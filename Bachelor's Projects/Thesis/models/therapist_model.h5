import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import SGD
import nltk
import json
import pickle

# Initialize NLP components
nltk.download('punkt', quiet=True)
lemmatizer = nltk.stem.WordNetLemmatizer()

# Load and preprocess training data
def load_and_process_data(intents_path='data/intents.json'):
    with open(intents_path) as file:
        intents = json.load(file)
    
    words = []
    classes = []
    documents = []
    ignore_chars = ['?', '!', '.', ',']

    for intent in intents['intents']:
        for pattern in intent['patterns']:
            tokens = nltk.word_tokenize(pattern)
            words.extend(tokens)
            documents.append((tokens, intent['tag']))
            if intent['tag'] not in classes:
                classes.append(intent['tag'])

    # Lemmatize and clean words
    words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_chars]
    words = sorted(list(set(words)))
    classes = sorted(list(set(classes)))

    # Save vocabulary
    pickle.dump(words, open('models/words.pkl', 'wb'))
    pickle.dump(classes, open('models/classes.pkl', 'wb'))

    return words, classes, documents

# Create training data
def create_training_data(words, classes, documents):
    training = []
    output_empty = [0] * len(classes)

    for doc in documents:
        bag = []
        pattern_words = [lemmatizer.lemmatize(word.lower()) for word in doc[0]]
        bag = [1 if word in pattern_words else 0 for word in words]
        
        output_row = list(output_empty)
        output_row[classes.index(doc[1])] = 1
        training.append([bag, output_row])

    np.random.shuffle(training)
    return np.array(training, dtype=object)

# Build and train model
def build_and_train_model():
    words, classes, documents = load_and_process_data()
    training_data = create_training_data(words, classes, documents)
    
    train_x = np.array(list(training_data[:,0]))
    train_y = np.array(list(training_data[:,1]))

    model = Sequential([
        Dense(128, input_shape=(len(words),), activation='relu'),
        Dropout(0.5),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(len(classes), activation='softmax')
    ])

    sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)
    model.compile(loss='categorical_crossentropy', 
                 optimizer=sgd, 
                 metrics=['accuracy'])

    model.fit(train_x, train_y, 
             epochs=200, 
             batch_size=5, 
             verbose=1)

    model.save('models/therapist_model.h5')
    print("Model saved as therapist_model.h5")

if __name__ == "__main__":
    build_and_train_model()
